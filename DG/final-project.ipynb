{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1TdpXXDLYDzHbaZEF2Cal5RzAtRBEY4cB","authorship_tag":"ABX9TyM+kbj+DJgLmEbnJ0On2eQP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F_miWDe77EXu"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import gc\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.max_rows', 500)\n","\n","sns.set()\n","sns.set_style('whitegrid')\n","sns.set_color_codes()"]},{"cell_type":"code","source":["train_trans = pd.read_csv(\"/content/drive/MyDrive/프로젝트/train_transaction.csv\")\n","train_idf = pd.read_csv(\"/content/drive/MyDrive/프로젝트/train_identity.csv\")\n","test_trans = pd.read_csv(\"/content/drive/MyDrive/프로젝트/test_transaction.csv\")\n","test_idf = pd.read_csv(\"/content/drive/MyDrive/프로젝트/test_identity.csv\")"],"metadata":{"id":"yfShc-m57qlc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["v_cols = [col for col in train_trans.columns if col.startswith('V')]\n","train_v = train_trans[v_cols]\n","test_v = test_trans[v_cols]\n","\n","# 원래 데이터프레임에서 V 컬럼 삭제\n","train_trans = train_trans.drop(columns=v_cols)\n","test_trans = test_trans.drop(columns=v_cols)"],"metadata":{"id":"2-FE7HOLOgyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_trans['TransactionDay'] = train_trans['TransactionDT'] // (24 * 60 * 60)\n","test_trans['TransactionDay'] = test_trans['TransactionDT'] // (24 * 60 * 60)"],"metadata":{"id":"Kg9M0P5J7veg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_idf.columns = [col.replace('-', '_') if 'id' in col else col for col in train_idf.columns]\n","test_idf.columns = [col.replace('-', '_') if 'id' in col else col for col in test_idf.columns]"],"metadata":{"id":"G52bGeFb7wzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in train_trans.columns:\n","    for i in range(1, 16):\n","      if i != 9:\n","        train_trans[f'D{i}N'] = train_trans['TransactionDay'] - train_trans[f'D{i}']\n","        test_trans[f'D{i}N'] = test_trans['TransactionDay'] - test_trans[f'D{i}']"],"metadata":{"id":"1PILBI4T7yPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n","train_trans['DT_M'] = train_trans['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n","test_trans['DT_M'] = test_trans['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n","train_trans['DT_M'] = (train_trans['DT_M'].dt.year-2017)*12 + train_trans['DT_M'].dt.month\n","test_trans['DT_M'] = (test_trans['DT_M'].dt.year-2017)*12 + test_trans['DT_M'].dt.month"],"metadata":{"id":"NCUf8enD76q4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id_feature = [ c for c in train_trans.columns if c.find('id_') !=-1]\n","v_feature = [ c for c in train_trans.columns if c.find('V') !=-1]\n","card_feature = [ c for c in train_trans.columns if c.find('card') !=-1]\n","C_feature = [ c for c in train_trans.columns if c.find('C') !=-1 and c != 'ProductCD']\n","D_feature = [ c for c in train_trans.columns if c.find('n') == -1 and c.find('D') !=-1 and c not in ['ProductCD','TransactionID','TransactionDT','DeviceType','DeviceInfo','TransactionDay', 'DT_M']]\n","Dn_feature = [ c for c in train_trans.columns if c.find('n') != -1 and c.find('D') !=-1 and c not in ['ProductCD','TransactionID','TransactionDT','DeviceType','DeviceInfo','TransactionDay', 'DT_M']]\n","M_feature = [ c for c in train_trans.columns if c.find('M') !=-1]"],"metadata":{"id":"h20OVjRy70TQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapping = {'T': 1, 'F': 0, 'M0': 0, 'M1': 1, 'M2': 2}\n","for col in M_feature:\n","    train_trans[col] = train_trans[col].map(mapping)\n","    test_trans[col] = test_trans[col].map(mapping)"],"metadata":{"id":"TNY47H8e8Elj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_FE(df1, df2, cols):\n","    for col in cols:\n","        df = pd.concat([df1[col],df2[col]])\n","        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n","        vc[-1] = -1\n","        nm = col+'_FE'\n","        df1[nm] = df1[col].map(vc)\n","        df1[nm] = df1[nm].astype('float32')\n","        df2[nm] = df2[col].map(vc)\n","        df2[nm] = df2[nm].astype('float32')\n","        print(nm,', ',end='')"],"metadata":{"id":"RsnK75Jn75Pf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_CB(col1,col2,df1=train_trans,df2=test_trans):\n","    nm = col1+'_'+col2\n","    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n","    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str)\n","    encode_LE(nm,verbose=False)\n","    print(nm,', ',end='')"],"metadata":{"id":"xizawAZQ79py"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_LE(col,train=train_trans,test=test_trans,verbose=True):\n","    df_comb = pd.concat([train[col],test[col]],axis=0)\n","    df_comb,_ = df_comb.factorize(sort=True)\n","    nm = col\n","    if df_comb.max()>32000:\n","        train[nm] = df_comb[:len(train)].astype('int32')\n","        test[nm] = df_comb[len(train):].astype('int32')\n","    else:\n","        train[nm] = df_comb[:len(train)].astype('int16')\n","        test[nm] = df_comb[len(train):].astype('int16')\n","    del df_comb; x=gc.collect()\n","    if verbose: print(nm,', ',end='')"],"metadata":{"id":"eoc1pxHx7-2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_AG(main_columns, uids, aggregations=['mean'], train_df = train_trans, test_df = test_trans,\n","              fillna=True, usena=False):\n","    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n","    for main_column in main_columns:\n","        for col in uids:\n","            for agg_type in aggregations:\n","                new_col_name = main_column+'_'+col+'_'+agg_type\n","                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n","                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n","                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n","                                                        columns={agg_type: new_col_name})\n","\n","                temp_df.index = list(temp_df[col])\n","                temp_df = temp_df[new_col_name].to_dict()\n","\n","                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n","                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n","\n","                if fillna:\n","                    train_df[new_col_name].fillna(-1,inplace=True)\n","                    test_df[new_col_name].fillna(-1,inplace=True)\n","\n","                print(\"'\"+new_col_name+\"'\",', ',end='')"],"metadata":{"id":"pklWV75F8AW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_AG2(main_columns, uids, train_df=train_trans, test_df=test_trans):\n","    for main_column in main_columns:\n","        for col in uids:\n","            comb = pd.concat([train_df[[col, main_column]],test_df[[col, main_column]]],axis=0)\n","            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n","            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n","            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n","            print(col+'_'+main_column+'_ct, ',end='')"],"metadata":{"id":"p47-azDb8Bu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combine_card_information(df1, df2):\n","    cols = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n","    df1['card_information'] = df1[cols].astype(str).agg('_'.join, axis=1)\n","    df2['card_information'] = df2[cols].astype(str).agg('_'.join, axis=1)\n","    # encode_LE('card_information', train=df1, test=df2, verbose=True)\n"],"metadata":{"id":"StrPKbwc7zzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combine_card_information(train_trans, test_trans)"],"metadata":{"id":"Ewv8MR-zD3Ke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def group_and_merge_stat(train_trans, test_trans, group_cols, target_col='TransactionAmt', agg_funcs=['mean', 'std']):\n","    agg_dict = {target_col: agg_funcs}\n","\n","    # 그룹화하여 통계 계산\n","    grouped = train_trans.groupby(group_cols).agg(agg_dict).reset_index()\n","    grouped.columns = group_cols + [f\"{'_'.join(group_cols)}_{target_col}_{agg_func}\" for agg_func in agg_funcs]\n","\n","    # train_df에 병합\n","    train_trans = train_trans.merge(grouped, on=group_cols, how='left')\n","\n","    # test_df에 병합\n","    grouped_test = test_trans.groupby(group_cols).agg(agg_dict).reset_index()\n","    grouped_test.columns = group_cols + [f\"{'_'.join(group_cols)}_{target_col}_{agg_func}\" for agg_func in agg_funcs]\n","    test_trans = test_trans.merge(grouped_test, on=group_cols, how='left')\n","\n","    return train_trans, test_trans\n"],"metadata":{"id":"XyC5dU4hNveO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# C 컬럼과 card_information 조합하여 통계 계산 함수\n","def group_and_merge_stat_with_card_info(train_trans, test_trans, C_cols, card_info_col, target_col='TransactionAmt', agg_funcs=['mean', 'std']):\n","    agg_dict = {target_col: agg_funcs}\n","\n","    for col in C_cols:\n","        # C 컬럼과 card_information 조합하여 그룹화\n","        group_cols = [col, card_info_col]\n","\n","        # 그룹화하여 통계 계산\n","        grouped = train_trans.groupby(group_cols).agg(agg_dict).reset_index()\n","        grouped.columns = group_cols + [f\"{col}_{card_info_col}_{target_col}_{agg_func}\" for agg_func in agg_funcs]\n","\n","        # train_df에 병합\n","        train_trans = train_trans.merge(grouped, on=group_cols, how='left')\n","\n","        # test_df에 병합\n","        grouped_test = test_trans.groupby(group_cols).agg(agg_dict).reset_index()\n","        grouped_test.columns = group_cols + [f\"{col}_{card_info_col}_{target_col}_{agg_func}\" for agg_func in agg_funcs]\n","        test_trans = test_trans.merge(grouped_test, on=group_cols, how='left')\n","\n","    return train_trans, test_trans"],"metadata":{"id":"22ARy508PvO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 그룹화 및 병합 수행\n","train_trans, test_trans = group_and_merge_stat_with_card_info(train_trans, test_trans, C_cols, 'card_information')"],"metadata":{"id":"MlH5RN7pY6-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 그룹화하여 통계 계산\n","grouped = train_trans.groupby(['card_information', 'addr1', 'D1N'])['TransactionAmt'].agg(['mean', 'std']).reset_index()\n","grouped2 = test_trans.groupby(['card_information', 'addr1', 'D1N'])['TransactionAmt'].agg(['mean', 'std']).reset_index()\n","\n","# 컬럼명 변경\n","grouped = grouped.rename(columns={'mean': 'card_information_addr1_D1N_TransactionAmt_mean', 'std': 'card_information_addr1_D1N_TransactionAmt_std'})\n","grouped2 = grouped2.rename(columns={'mean': 'card_information_addr1_D1N_TransactionAmt_mean', 'std': 'card_information_addr1_D1N_TransactionAmt_std'})\n","\n","# 원래 데이터프레임에 병합\n","train_trans = train_trans.merge(grouped, on=['card_information', 'addr1', 'D1N'], how='left')\n","test_trans = test_trans.merge(grouped2, on=['card_information', 'addr1', 'D1N'], how='left')"],"metadata":{"id":"UzUVq--9FZb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uid_cols = ['card_information', 'addr1']\n","train_trans, test_trans = group_and_merge_stat(train_trans, test_trans, uid_cols, 'dist1')"],"metadata":{"id":"_0QI6bSMUyZI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 그룹화 및 병합 수행\n","email_cols = ['P_emaildomain', 'R_emaildomain']\n","uid_cols = ['card_information']\n","\n","# P_emaildomain과 card_information 결합하여 통계 계산\n","for email_col in email_cols:\n","    train_trans, test_trans = group_and_merge_stat(train_trans, test_trans, [email_col] + uid_cols, 'TransactionAmt')"],"metadata":{"id":"6p-RF7qMX_ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 그룹화 및 병합 수행\n","uid_cols = ['card_information', 'addr1', 'DT_M']\n","\n","# 카드 정보, addr1, DT_M을 결합하여 통계 계산\n","train_trans, test_trans = group_and_merge_stat(train_trans, test_trans, uid_cols, 'TransactionAmt')"],"metadata":{"id":"gnieJuXWcaMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불필요한 컬럼 제거\n","drop_cols = ['P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6']\n","train_trans.drop(columns=drop_cols, inplace=True)\n","test_trans.drop(columns=drop_cols, inplace=True)"],"metadata":{"id":"Ir4RhhRFe8XI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder, StandardScaler\n","\n","def encode_LE(columns, train_df, test_df, verbose=True):\n","    for col in columns:\n","        df_comb = pd.concat([train_df[col], test_df[col]], axis=0)\n","        df_comb, _ = df_comb.factorize(sort=True)\n","        nm = col\n","        if df_comb.max() > 32000:\n","            train_df[nm] = df_comb[:len(train_df)].astype('int32')\n","            test_df[nm] = df_comb[len(train_df):].astype('int32')\n","        else:\n","            train_df[nm] = df_comb[:len(train_df)].astype('int16')\n","            test_df[nm] = df_comb[len(train_df):].astype('int16')\n","        del df_comb\n","        gc.collect()\n","        if verbose:\n","            print(nm, ', ', end='')\n","\n","# 라벨 인코딩이 필요한 컬럼\n","label_cols = ['ProductCD', 'card_information', 'TransactionID']\n","encode_LE(label_cols, train_trans, test_trans)\n","\n","\n","# 스탠다드 스케일링이 필요한 수치형 컬럼\n","scale_cols = [\n","    'TransactionDT', 'TransactionAmt', 'C1_card_information_TransactionAmt_mean', 'C1_card_information_TransactionAmt_std',\n","    'C2_card_information_TransactionAmt_mean', 'C2_card_information_TransactionAmt_std', 'C3_card_information_TransactionAmt_mean', 'C3_card_information_TransactionAmt_std',\n","    'C4_card_information_TransactionAmt_mean', 'C4_card_information_TransactionAmt_std', 'C5_card_information_TransactionAmt_mean', 'C5_card_information_TransactionAmt_std',\n","    'C6_card_information_TransactionAmt_mean', 'C6_card_information_TransactionAmt_std', 'C7_card_information_TransactionAmt_mean', 'C7_card_information_TransactionAmt_std',\n","    'C8_card_information_TransactionAmt_mean', 'C8_card_information_TransactionAmt_std', 'C9_card_information_TransactionAmt_mean', 'C9_card_information_TransactionAmt_std',\n","    'C10_card_information_TransactionAmt_mean', 'C10_card_information_TransactionAmt_std', 'card_information_addr1_D1N_TransactionAmt_mean', 'card_information_addr1_D1N_TransactionAmt_std',\n","    'card_information_addr1_dist1_mean', 'card_information_addr1_dist1_std', 'P_emaildomain_card_information_TransactionAmt_mean', 'P_emaildomain_card_information_TransactionAmt_std',\n","    'R_emaildomain_card_information_TransactionAmt_mean', 'R_emaildomain_card_information_TransactionAmt_std', 'card_information_addr1_DT_M_TransactionAmt_mean', 'card_information_addr1_DT_M_TransactionAmt_std'\n","]\n","\n","scaler = StandardScaler()\n","train_trans[scale_cols] = scaler.fit_transform(train_trans[scale_cols])\n","test_trans[scale_cols] = scaler.transform(test_trans[scale_cols])"],"metadata":{"id":"qiXwxu4naoAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import lightgbm as lgb\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.metrics import roc_auc_score\n","\n","# # LightGBM 모델 학습\n","# target_col = 'isFraud'\n","# X_train = train_trans.drop(columns=[target_col])\n","# y_train = train_trans[target_col]\n","# X_test = test_trans.drop(columns=[target_col], errors='ignore')\n","\n","# params = {\n","#     'objective': 'binary',\n","#     'metric': 'auc',\n","#     'num_iterations': 100,\n","# }\n","\n","# train_data = lgb.Dataset(X_train, label=y_train)\n","# lgbm_model = lgb.train(params, train_data)\n","\n","# # 테스트 데이터에 대한 예측\n","# y_test_pred = lgbm_model.predict(X_test, num_iteration=lgbm_model.best_iteration)\n","\n","# # 피처 중요도 출력\n","# importance = lgbm_model.feature_importance(importance_type='gain')\n","# feature_names = X_train.columns\n","# feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importance})\n","# feature_importances = feature_importances.sort_values(by='importance', ascending=False)"],"metadata":{"id":"5aXxZfPYZFsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 피처 중요도 시각화\n","# plt.figure(figsize=(12, 20))  # 그래프 크기를 키움\n","# plt.barh(feature_importances['feature'], feature_importances['importance'])\n","# plt.xlabel('Importance')\n","# plt.ylabel('Feature')\n","# plt.title('Feature Importances')\n","# plt.gca().invert_yaxis()\n","# plt.xticks(fontsize=10)\n","# plt.yticks(fontsize=8)\n","# plt.show()"],"metadata":{"id":"Q1k_qrykd9Bl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","\n","# 데이터셋 나누기\n","target_col = 'isFraud'\n","X = train_trans.drop(columns=[target_col])\n","y = train_trans[target_col]\n","\n","# 학습 데이터와 검증 데이터 나누기\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=47)\n","\n","# LightGBM 파라미터 설정\n","params = {\n","    'num_leaves': 491,\n","    'min_child_weight': 0.03454472573214212,\n","    'feature_fraction': 0.3797454081646243,\n","    'bagging_fraction': 0.4181193142567742,\n","    'min_data_in_leaf': 106,\n","    'objective': 'binary',\n","    'max_depth': -1,\n","    'learning_rate': 0.006883242363721497,\n","    \"boosting_type\": \"gbdt\",\n","    \"bagging_seed\": 11,\n","    \"metric\": 'auc',\n","    \"verbosity\": -1,\n","    'reg_alpha': 0.3899927210061127,\n","    'reg_lambda': 0.6485237330340494,\n","    'random_state': 47\n","}\n","\n","# LightGBM 데이터셋 생성\n","train_data = lgb.Dataset(X_train, label=y_train)\n","valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n","\n","# 모델 학습\n","lgbm_model = lgb.train(\n","    params,\n","    train_data,\n","    num_boost_round=500,\n","    valid_sets=[train_data, valid_data],\n","    valid_names=['train', 'valid'],\n","    callbacks=[\n","        lgb.early_stopping(stopping_rounds=50),\n","        lgb.log_evaluation(period=50)\n","    ]\n",")\n","\n","# 검증 데이터에 대한 예측\n","y_valid_pred = lgbm_model.predict(X_valid, num_iteration=lgbm_model.best_iteration)\n","\n","# AUC 점수 계산\n","auc_score = roc_auc_score(y_valid, y_valid_pred)\n","print(f'Validation AUC: {auc_score}')\n","\n","# 피처 중요도 출력\n","importance = lgbm_model.feature_importance(importance_type='gain')\n","feature_names = X_train.columns\n","feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importance})\n","feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n","\n","# 상위 20개의 피처 중요도만 선택\n","top_n = 20\n","top_features = feature_importances.head(top_n)\n","\n","# 피처 중요도 시각화\n","plt.figure(figsize=(12, 8))  # 그래프 크기 조정\n","plt.barh(top_features['feature'], top_features['importance'])\n","plt.xlabel('Importance')\n","plt.ylabel('Feature')\n","plt.title(f'Top {top_n} Feature Importances')\n","plt.gca().invert_yaxis()\n","plt.xticks(fontsize=10)\n","plt.yticks(fontsize=10)\n","plt.show()"],"metadata":{"id":"NnCHA4BLgi5Z"},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic column V301 id와 묶어서\n",
    "# productCD  transactionamt 묶어서\n",
    "# card4\n",
    "# Transform id_30, id_31¶\n",
    "# addr2 특정 지역 무적권 사기\n",
    "# id 30 id31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import hashlib\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "#        else:\n",
    "#            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transaction = pd.read_csv('train_transaction.csv', index_col='TransactionID')\n",
    "train_identity = pd.read_csv('train_identity.csv', index_col='TransactionID')\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "del train_transaction, train_identity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transaction = pd.read_csv('test_transaction.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('test_identity.csv', index_col='TransactionID')\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "del test_transaction, test_identity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns = test.columns.str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = ['TransactionDT',\n",
    " 'TransactionAmt','card1','card2','card3','card5',\n",
    " 'addr1','addr2',\n",
    " 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    " 'D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15',\n",
    " \n",
    " 'V307',\n",
    " \n",
    " 'ProductCD',\n",
    " 'card4',\n",
    " 'card6',\n",
    " 'P_emaildomain',\n",
    " 'R_emaildomain', \n",
    " 'dist1','id_30','id_31', 'id_32', 'id_33', 'id_34',\n",
    " 'DeviceType','DeviceInfo',]\n",
    "\n",
    "len(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[selected_features + ['isFraud']]\n",
    "test = test[selected_features ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-999)\n",
    "test = test.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Card_device 조합에서 V301 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['card1'] = train['card1'].fillna(0)\n",
    "train['card2'] = train['card2'].fillna(0)\n",
    "train['card3'] = train['card3'].fillna(0)\n",
    "train['card5'] = train['card5'].fillna(0)\n",
    "train['card4'] = train['card4'].fillna('nan')\n",
    "train['card6'] = train['card6'].fillna('nan')\n",
    "\n",
    "test['card1'] = test['card1'].fillna(0)\n",
    "test['card2'] = test['card2'].fillna(0)\n",
    "test['card3'] = test['card3'].fillna(0)\n",
    "test['card5'] = test['card5'].fillna(0)\n",
    "test['card4'] = test['card4'].fillna('nan')\n",
    "test['card6'] = test['card6'].fillna('nan')\n",
    "\n",
    "def card_info_hash(x):\n",
    "    s = (str(int(x['card1']))+\n",
    "         str(int(x['card2']))+\n",
    "         str(int(x['card3']))+\n",
    "         str(x['card4'])+\n",
    "         str(int(x['card5']))+\n",
    "         str(x['card6']))\n",
    "    h = hashlib.sha256(s.encode('utf-8')).hexdigest()[0:15]\n",
    "    return h\n",
    "\n",
    "def device_hash(x):\n",
    "    s =  str(x['id_30'])+str(x['id_31'])+str(x['id_32'])+str(x['id_33'])+str( x['DeviceType'])+ str(x['DeviceInfo'])\n",
    "    h = hashlib.sha256(s.encode('utf-8')).hexdigest()[0:15]\n",
    "    return h\n",
    "\n",
    "train['card_hash'] = train.apply(lambda x: card_info_hash(x), axis=1   )\n",
    "train['device_hash'] = train.apply(lambda x: device_hash(x), axis=1   )\n",
    "\n",
    "test['card_hash'] = test.apply(lambda x: card_info_hash(x), axis=1   )\n",
    "test['device_hash'] = test.apply(lambda x: device_hash(x), axis=1   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['V307_diff'] = train['V307'].diff().shift(-1)\n",
    "train['difference'] = train['V307_diff'] - train['TransactionAmt']\n",
    "\n",
    "test['v307_diff'] = test['V307'].diff().shift(-1)\n",
    "test['difference'] = test['v307_diff'] - test['TransactionAmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 256.81 MB\n",
      "Memory usage after optimization is: 115.45 MB\n",
      "Decreased by 55.0%\n",
      "Memory usage of dataframe is 216.48 MB\n",
      "Memory usage after optimization is: 98.58 MB\n",
      "Decreased by 54.5%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract target variable¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 범주형 열을 숫자형으로 변환\n",
    "def encode_categorical_columns(df):\n",
    "    label_encoders = {}\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "    return df, label_encoders\n",
    "\n",
    "# train 데이터 인코딩\n",
    "train, _ = encode_categorical_columns(train)\n",
    "\n",
    "# test 데이터 인코딩\n",
    "test_encoded, _ = encode_categorical_columns(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in train but not in test_encoded: {'isFraud', 'V307_diff'}\n"
     ]
    }
   ],
   "source": [
    "train_columns = set(train.columns)\n",
    "test_columns = set(test_encoded.columns)\n",
    "\n",
    "column_difference = train_columns - test_columns\n",
    "\n",
    "print(\"Columns in train but not in test_encoded:\", column_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in train: 56\n",
      "Number of columns in test_encoded: 55\n"
     ]
    }
   ],
   "source": [
    "train_columns = train.shape[1]\n",
    "test_columns = test_encoded.shape[1]\n",
    "\n",
    "print(\"Number of columns in train:\", train_columns)\n",
    "print(\"Number of columns in test_encoded:\", test_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['isFraud']\n",
    "X = train.drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 15497, number of negative: 427408\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10343\n",
      "[LightGBM] [Info] Number of data points in the train set: 442905, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034989 -> initscore=-3.317093\n",
      "[LightGBM] [Info] Start training from score -3.317093\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.940915\tvalid_1's auc: 0.925415\n",
      "[200]\ttraining's auc: 0.960158\tvalid_1's auc: 0.938779\n",
      "[300]\ttraining's auc: 0.97071\tvalid_1's auc: 0.945266\n",
      "[400]\ttraining's auc: 0.978117\tvalid_1's auc: 0.949553\n",
      "[500]\ttraining's auc: 0.983636\tvalid_1's auc: 0.952362\n",
      "[600]\ttraining's auc: 0.987071\tvalid_1's auc: 0.954919\n",
      "[700]\ttraining's auc: 0.989964\tvalid_1's auc: 0.956625\n",
      "[800]\ttraining's auc: 0.992365\tvalid_1's auc: 0.958636\n",
      "[900]\ttraining's auc: 0.993798\tvalid_1's auc: 0.959942\n",
      "[1000]\ttraining's auc: 0.994996\tvalid_1's auc: 0.961387\n",
      "[1100]\ttraining's auc: 0.996025\tvalid_1's auc: 0.962497\n",
      "[1200]\ttraining's auc: 0.996802\tvalid_1's auc: 0.963316\n",
      "[1300]\ttraining's auc: 0.997487\tvalid_1's auc: 0.963976\n",
      "[1400]\ttraining's auc: 0.997988\tvalid_1's auc: 0.964443\n",
      "[1500]\ttraining's auc: 0.998495\tvalid_1's auc: 0.965045\n",
      "[1600]\ttraining's auc: 0.998819\tvalid_1's auc: 0.965582\n",
      "[1700]\ttraining's auc: 0.999046\tvalid_1's auc: 0.965852\n",
      "[1800]\ttraining's auc: 0.999238\tvalid_1's auc: 0.966377\n",
      "[1900]\ttraining's auc: 0.999408\tvalid_1's auc: 0.966675\n",
      "[2000]\ttraining's auc: 0.999518\tvalid_1's auc: 0.966883\n",
      "[2100]\ttraining's auc: 0.999611\tvalid_1's auc: 0.967064\n",
      "[2200]\ttraining's auc: 0.999686\tvalid_1's auc: 0.967326\n",
      "[2300]\ttraining's auc: 0.999765\tvalid_1's auc: 0.967531\n",
      "[2400]\ttraining's auc: 0.999806\tvalid_1's auc: 0.967687\n",
      "[2500]\ttraining's auc: 0.999841\tvalid_1's auc: 0.96765\n",
      "Early stopping, best iteration is:\n",
      "[2442]\ttraining's auc: 0.999821\tvalid_1's auc: 0.967757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "# train, valid split\n",
    "X_tr, X_v, y_tr, y_v = train_test_split(X, y, test_size=0.25,\n",
    "random_state=2024,\n",
    "stratify=y)\n",
    "\n",
    "# 데이터 셋 생성\n",
    "train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "valid_data = lgb.Dataset(X_v, label=y_v)\n",
    "\n",
    "\n",
    "params = {\n",
    "'objective': 'binary',\n",
    "'metric': 'auc'\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=100),\n",
    "lgb.log_evaluation(period=100)]\n",
    "clf = lgb.train(params, train_data,\n",
    "num_boost_round = 10000,\n",
    "valid_sets = [train_data, valid_data],\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test 제출\n",
    "preds = clf.predict(test_encoded)\n",
    "result = pd.read_csv(\"sample_submission.csv\")\n",
    "result ['isFraud'] = preds\n",
    "result .to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
